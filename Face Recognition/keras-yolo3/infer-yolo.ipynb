{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aAaIheCzIbBN",
    "outputId": "39baf7c2-b443-4a2a-ada2-3d7b8ff798f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting yolo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile yolo.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Class definition of YOLO_v3 style detection model on image and video\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "##%matplotlib inline\n",
    "#pil_im = Image.open('data/empire.jpg', 'r')\n",
    "#imshow(np.asarray(pil_im))t\n",
    "#matplotlib notebook\n",
    "import collections\n",
    "import colorsys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "#from google.colab.patches import cv2_imshow \n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "#from keras import backend as K\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#from tensorflow.keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from keras import backend as K\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image\n",
    "import os\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import preprocessing\n",
    "import cv2\n",
    "#import nms\n",
    "#The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
    "\n",
    "class YOLO(object):\n",
    "    _defaults = {\n",
    "        \"model_path\": 'model_data/faces.h5',\n",
    "        \"anchors_path\": 'model_data/yolo_anchors.txt',\n",
    "        \"classes_path\": 'model_data/faces.txt',\n",
    "        \"score\" : 0.2,\n",
    "        \"iou\" : 0.45,\n",
    "        \"model_image_size\" : (416, 416),\n",
    "        \"gpu_num\" : 1,\n",
    "    }\n",
    "    \"\"\"You can also call class_foo using the class. In fact, if you define \n",
    "    something to be a classmethod, it is probably because you intend to call it\n",
    "    from the class rather than from a class instance. A.foo(1) would have raised\n",
    "    a TypeError, but A.class_foo(1) works just fine:\n",
    "\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def get_defaults(cls, n):\n",
    "        if n in cls._defaults:\n",
    "            return cls._defaults[n]\n",
    "        else:\n",
    "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(self._defaults) # set up default values\n",
    "        self.__dict__.update(kwargs) # and update with user overrides\n",
    "        self.class_names = self._get_class()\n",
    "        self.anchors = self._get_anchors()\n",
    "        self.sess = K.get_session()\n",
    "        self.boxes, self.scores, self.classes = self.generate()\n",
    "\n",
    "    def _get_class(self):\n",
    "        classes_path = os.path.expanduser(self.classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        anchors_path = os.path.expanduser(self.anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    def generate(self):\n",
    "        model_path = os.path.expanduser(self.model_path)\n",
    "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
    "\n",
    "        # Load model, or construct model and load weights.\n",
    "        num_anchors = len(self.anchors)\n",
    "        num_classes = len(self.class_names)\n",
    "        is_tiny_version = num_anchors==6 # default setting\n",
    "        \n",
    "        try:\n",
    "            self.yolo_model = load_model(model_path, compile=False)\n",
    "        except:\n",
    "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
    "            if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
    "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
    "        else:\n",
    "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
    "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
    "                'Mismatch between model and given anchor and class sizes'\n",
    "\n",
    "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
    "\n",
    "        # Generate colors for drawing bounding boxes.\n",
    "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
    "                      for x in range(len(self.class_names))]\n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        self.colors = list(\n",
    "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
    "                self.colors))\n",
    "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "        np.random.seed(None)  # Reset seed to default.\n",
    "\n",
    "        # Generate output tensor targets for filtered bounding boxes.\n",
    "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
    "        if self.gpu_num>=2:\n",
    "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
    "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
    "                len(self.class_names), self.input_image_shape,\n",
    "                score_threshold=self.score, iou_threshold=self.iou)\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def detect_image(self, image):\n",
    "        start = timer()\n",
    "\n",
    "        if self.model_image_size != (None, None):\n",
    "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
    "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
    "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
    "        else:\n",
    "            new_image_size = (image.width - (image.width % 32),\n",
    "                              image.height - (image.height % 32))\n",
    "            boxed_image = letterbox_image(image, new_image_size)\n",
    "        image_data = np.array(boxed_image, dtype='float32')\n",
    "\n",
    "        print(image_data.shape)\n",
    "        image_data /= 255.\n",
    "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "\n",
    "        out_boxes, out_scores, out_classes = self.sess.run(\n",
    "            [self.boxes, self.scores, self.classes],\n",
    "            feed_dict={\n",
    "                self.yolo_model.input: image_data,\n",
    "                self.input_image_shape: [image.size[1], image.size[0]],\n",
    "                K.learning_phase(): 0\n",
    "            })\n",
    "\n",
    "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
    "\n",
    "        font = ImageFont.truetype(font='font/FiraMono-Medium.otf',\n",
    "                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "        thickness = (image.size[0] + image.size[1]) // 300\n",
    "        \n",
    "        \n",
    "        bb_list = []\n",
    "        detections = []\n",
    "        scores = np.zeros((len(out_classes), 1))\n",
    "        boxes = np.zeros((len(out_classes), 4))\n",
    "        # Common Values of nms_max_overlap are between 0.3 to 0.5\n",
    "        nms_max_overlap = 0.3\n",
    "        #Detection = collections.namedtuple('Detections', 'tlwh confidence label') \n",
    "        for i, c in reversed(list(enumerate(out_classes))):\n",
    "            \n",
    "            predicted_class = self.class_names[c]\n",
    "            if predicted_class != 'front' and predicted_class != 'side' and predicted_class != 'down' and predicted_class != 'head':\n",
    "              \"\"\"\n",
    "              Remember deletion of the box means deletion in box means deletion\n",
    "              in boxes and scores arrays.\n",
    "              \"\"\"\n",
    "              boxes = np.delete(boxes, i, axis = 0)\n",
    "              scores = np.delete(scores, i, axis=0)\n",
    "              continue\n",
    "            box = out_boxes[i]\n",
    "            score = out_scores[i]\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "\n",
    "            \n",
    "\n",
    "            top, left, bottom, right = box\n",
    "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
    "            print(label, (left, top), (right, bottom))\n",
    "            \n",
    "            width = right - left\n",
    "            height = bottom - top\n",
    "            centerX = (left + right) // 2\n",
    "            centerY = (top + bottom) // 2\n",
    "            a = height / width\n",
    "            \n",
    "            #boxes[i] = np.array([top, left, width, height])\n",
    "            boxes[i] = np.array([left, top, right, bottom])\n",
    "            scores[i] = np.array([score])\n",
    "            #detections.append([' ', str(centerX), ' ', str(centerY), ' ', str(a), ' ',  str(height) ])\n",
    "            #bb_list.append([' ', str(left), ' ', str(top), ' ', str(right), ' ', str(bottom)])\n",
    "\n",
    "\n",
    "        scores = scores[:,0]      \n",
    "        #import pdb; pdb.set_trace()\n",
    "        if  len(boxes) == 0:\n",
    "            return (image, bb_list, detections)\n",
    "        indices = preprocessing.non_max_suppression_slow(\n",
    "        boxes, nms_max_overlap, scores)\n",
    "        \n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        #xx = cv2.dnn.NMSBoxes(boxes, scores, 0.3, 0.3)\n",
    "        \n",
    "        #print('OpenCV nms indices: {}'.format(xx))\n",
    "        #import pdb#; pdb.pm()\n",
    "        \n",
    "        #print(\"Pypi's nms indices: {}\".format(nms.nms.boxes(boxes, scores)))\n",
    "         \n",
    "        #real_detections = [detections[i] for i in indices]\n",
    "\n",
    "\n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        for indice in indices:\n",
    "            centerX = (indice[0] + indice[2]) //2\n",
    "            centerY = (indice[1] + indice[3]) // 2\n",
    "            height = (indice[3] - indice[1])\n",
    "            width = (indice[2] - indice[0])\n",
    "            a = height/width\n",
    "            bb_list.append([' ', str(indice[0]), ' ', str(indice[1]), ' ', str(indice[2]), ' ', str(indice[3])])\n",
    "            detections.append([' ', str(centerX), ' ', str(centerY), ' ', str(a), ' ', str(height) ])\n",
    "        \"\"\"\n",
    "        print('Boxes: {}'.format(boxes))\n",
    "        print('Scores: {}'.format(scores))\n",
    "        print('bb_list: ', bb_list)\n",
    "        print('Length of bb_list: {}'.format(len(bb_list)))\n",
    "        print('Length of Indices: {}'.format(len(indices)))\n",
    "        print('Length of boxes: {}'.format(len(boxes)))\n",
    "        print('Length of detections: {}'.format(len(detections)))\n",
    "        print('Indices: {}'.format(indices))\n",
    "        print('Detections {}'.format(detections))\n",
    "        \n",
    "        if len(indices) == 1:\n",
    "          bb_list = [bb_list[indices[0]]]\n",
    "          detections = [detections[indices[0]]]\n",
    "        else:\n",
    "          bb_list = [bb_list[i] for i in indices]\n",
    "          detections = [detections[i] for i in indices]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for i in range(len(indices)):\n",
    "            label = '{} {:.2f}'.format(predicted_class, float(scores[i]))\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            label_size = draw.textsize(label, font)\n",
    "            top = int(indices[i][1])\n",
    "            left = int(indices[i][0])\n",
    "            bottom = int(indices[i][3])\n",
    "            right = int(indices[i][2])\n",
    "\n",
    "            # My kingdom for a good redistributable image drawing library.\n",
    "            for i in range(thickness):\n",
    "\n",
    "                if top - label_size[1] >= 0:\n",
    "                    text_origin = np.array([left, top - label_size[1]])\n",
    "                else:\n",
    "                    text_origin = np.array([left, top + 1])\n",
    "                draw.rectangle(\n",
    "                    [left + i, top + i, right - i, bottom - i],\n",
    "                    outline=self.colors[c])\n",
    "            draw.rectangle(\n",
    "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
    "                fill=self.colors[c])\n",
    "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
    "            del draw\n",
    "        end = timer()\n",
    "        #print(end - start)\n",
    "        out_classes = list(reversed(out_classes))\n",
    "        return (image, bb_list, detections , out_classes)\n",
    "     \n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "\n",
    "def detect_video(yolo, video_path, output_path=\"\"):\n",
    "    import cv2\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    if not vid.isOpened():\n",
    "        raise IOError(\"Couldn't open webcam or video\")\n",
    "    video_FourCC    = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n",
    "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    isOutput = True if output_path != \"\" else False\n",
    "    if isOutput:\n",
    "        print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n",
    "        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
    "    accum_time = 0\n",
    "    curr_fps = 0\n",
    "    fps = \"FPS: ??\"\n",
    "    prev_time = timer()\n",
    "    while True:\n",
    "        return_value, frame = vid.read()\n",
    "        if not return_value:\n",
    "          break\n",
    "        image = Image.fromarray(frame)\n",
    "        image = yolo.detect_image(image)\n",
    "        result = np.asarray(image)\n",
    "        curr_time = timer()\n",
    "        exec_time = curr_time - prev_time\n",
    "        prev_time = curr_time\n",
    "        accum_time = accum_time + exec_time\n",
    "        curr_fps = curr_fps + 1\n",
    "        if accum_time > 1:\n",
    "            accum_time = accum_time - 1\n",
    "            fps = \"FPS: \" + str(curr_fps)\n",
    "            curr_fps = 0\n",
    "        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n",
    "        #cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL)\n",
    "        print(type(result))\n",
    "        cv2.imshow(\"result\", result)\n",
    "        #imshow(result)\n",
    "        if isOutput:\n",
    "            out.write(result)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    out.release()\n",
    "    yolo.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wr4inAuSPesP",
    "outputId": "bf3d636a-42a1-41dc-ccea-bc9b987f8123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting yolo_video_counter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile yolo_video_counter.py \n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import argparse\n",
    "from yolo import YOLO, detect_video\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Edit this path for the directory of images.\n",
    "mylist = sorted(os.listdir(os.path.join(os.getcwd(),'test')))\n",
    "#print(mylist)\n",
    "    \n",
    "def detect_img(arg, yolo):\n",
    "    \"\"\"\n",
    "    This is the controlling function which is invoked once the file is called from terminal with arguments\n",
    "    This function loads and runs the YOLO model and calls the other functions to implement the counter\n",
    "    detection logic\n",
    "    \"\"\"\n",
    "    file2 = open('model_data/out_classes.txt' , 'w')\n",
    "    file2.close()\n",
    "    bb_coords = []\n",
    "    detections = []    \n",
    "    for m, img in enumerate(arg):\n",
    "        #img = input('Input image filename:')\n",
    "        try:\n",
    "            # Change the image filename\n",
    "            image = Image.open(os.path.join(os.getcwd(), 'test', img))\n",
    "            #print(os.path.join(os.getcwd(), 'test', img))\n",
    "        except:         \n",
    "            print('Open Error! Try again!')\n",
    "            continue\n",
    "        else:\n",
    "            r_image, bb_list, detections_list , out_classes = yolo.detect_image(image)\n",
    "            i = 0\n",
    "            person_list = []\n",
    "            bb_coords.append(bb_list)\n",
    "            detections.append(detections_list)\n",
    "            r_image.save(os.path.join(os.getcwd(),\"output_folder/detection{}.png\".format(m)),\"PNG\")\n",
    "        file1 = open('model_data/counter_bb_coords.txt' , 'w')\n",
    "        file2 = open('model_data/out_classes.txt' , 'a')\n",
    "        file2.write(str(out_classes))\n",
    "        file2.write('\\n')\n",
    "        for coords in bb_coords:            \n",
    "            for coord in coords:\n",
    "                file1.writelines(coord)\n",
    "            file1.write('\\n')\n",
    "        file1.close()\n",
    "        file2.close()\n",
    "FLAGS = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # class YOLO defines the default value, so suppress any default here\n",
    "    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)\n",
    "    '''\n",
    "    Command line options\n",
    "    '''\n",
    "    parser.add_argument(\n",
    "        '--model', type=str,\n",
    "        help='path to model weight file, default ' + YOLO.get_defaults(\"model_path\")\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--anchors', type=str,\n",
    "        help='path to anchor definitions, default ' + YOLO.get_defaults(\"anchors_path\")\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--classes', type=str,\n",
    "        help='path to class definitions, default ' + YOLO.get_defaults(\"classes_path\")\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--gpu_num', type=int,\n",
    "        help='Number of GPU to use, default ' + str(YOLO.get_defaults(\"gpu_num\"))\n",
    "    )\n",
    "\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--image', default=False, action=\"store_true\",\n",
    "        help='Image detection mode, will ignore all positional arguments' \n",
    "    )\n",
    "    '''\n",
    "    Command line positional arguments -- for video detection mode\n",
    "    '''\n",
    "    parser.add_argument(\n",
    "        \"--input\", nargs='?', type=str,required=False,default='./path2your_video',\n",
    "        help = \"Video input path\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output\", nargs='?', type=str, default=\"\",\n",
    "        help = \"[Optional] Video output path\"\n",
    "    )\n",
    "\n",
    "    FLAGS = parser.parse_args()\n",
    "\n",
    "    if FLAGS.image:\n",
    "        \"\"\"\n",
    "        Image detection mode, disregard any remaining command line arguments\n",
    "        \"\"\"\n",
    "        print(\"Image detection mode\" , file = sys.stderr)\n",
    "        if \"input\" in FLAGS:\n",
    "            print(\" Ignoring remaining command line arguments: \" + FLAGS.input + \",\" + FLAGS.output,\n",
    "                 file = sys.stderr)\n",
    "        detect_img(mylist, YOLO(**vars(FLAGS)))\n",
    "    elif \"input\" in FLAGS:\n",
    "        detect_video(YOLO(**vars(FLAGS)), FLAGS.input, FLAGS.output)\n",
    "    else:\n",
    "        print(\"Must specify at least video_input_path.  See usage with --help.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n",
      "Image detection mode\n",
      " Ignoring remaining command line arguments: counter01881.jpg,output_folder/\n",
      "WARNING:tensorflow:From /home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:180: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-10-31 01:17:36.683343: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-10-31 01:17:36.687505: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2019-10-31 01:17:36.740463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-31 01:17:36.740768: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4170e90 executing computations on platform CUDA. Devices:\n",
      "2019-10-31 01:17:36.740782: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2019-10-31 01:17:36.761395: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz\n",
      "2019-10-31 01:17:36.761695: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4579270 executing computations on platform Host. Devices:\n",
      "2019-10-31 01:17:36.761734: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-10-31 01:17:36.762094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-31 01:17:36.762311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\n",
      "pciBusID: 0000:01:00.0\n",
      "2019-10-31 01:17:36.762396: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64\n",
      "2019-10-31 01:17:36.762442: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64\n",
      "2019-10-31 01:17:36.762485: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64\n",
      "2019-10-31 01:17:36.762526: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64\n",
      "2019-10-31 01:17:36.762568: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64\n",
      "2019-10-31 01:17:36.762609: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64\n",
      "2019-10-31 01:17:36.765286: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-10-31 01:17:36.765306: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\n",
      "2019-10-31 01:17:36.765321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-10-31 01:17:36.765327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2019-10-31 01:17:36.765332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "WARNING:tensorflow:From /home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-31 01:17:36.843793: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "WARNING:tensorflow:From /home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1944: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "model_data/faces.h5 model, anchors, and classes loaded.\n",
      "WARNING:tensorflow:From /home/dk-tanmay/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "(416, 416, 3)\n",
      "Found 3 boxes for img\n",
      "head 0.85 (1168, 278) (1230, 367)\n",
      "front 0.33 (125, 11) (148, 40)\n",
      "front 0.40 (731, 14) (761, 40)\n"
     ]
    }
   ],
   "source": [
    "#import time\n",
    "#start = time.clock()\n",
    "!python3 yolo_video_counter.py --model logs/000/old/new_person.h5 --anchors model_data/tiny_yolo_anchors.txt --classes model_data/people_tracking_classes.txt --image --input counter01881.jpg --output output_folder/\n",
    "#end = time.clock()\n",
    "#print(\"Time Elapsed = \" , end - start)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xoCknHCLznpb"
   ],
   "name": "qqwweee_keras_yolov3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
